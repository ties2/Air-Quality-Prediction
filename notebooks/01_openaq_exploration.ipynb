{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - OpenAQ Data Exploration\n",
    "\n",
    "This notebook explores the OpenAQ dataset for air quality prediction.\n",
    "\n",
    "We'll cover:\n",
    "1. Connecting to OpenAQ API v3\n",
    "2. Exploring available cities and parameters\n",
    "3. Visualizing pollution patterns\n",
    "4. Preparing data for continual learning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data.openaq_client import OpenAQClient, OpenAQDataFetcher\n",
    "\n",
    "# Set your API key\n",
    "# os.environ['OPENAQ_API_KEY'] = 'your-api-key-here'\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to OpenAQ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def explore_api():\n",
    "    async with OpenAQClient() as client:\n",
    "        # Get available parameters\n",
    "        params = await client.get_parameters()\n",
    "        print(\"Available Parameters:\")\n",
    "        for p in params:\n",
    "            print(f\"  - {p['name']} (ID: {p['id']}): {p.get('description', 'N/A')}\")\n",
    "        \n",
    "        # Get countries\n",
    "        countries = await client.get_countries()\n",
    "        print(f\"\\nTotal countries: {len(countries)}\")\n",
    "        \n",
    "        # Top countries by location count\n",
    "        top_countries = sorted(countries, key=lambda x: x.get('locations', 0), reverse=True)[:10]\n",
    "        print(\"\\nTop 10 countries by monitoring locations:\")\n",
    "        for c in top_countries:\n",
    "            print(f\"  - {c['name']}: {c.get('locations', 0)} locations\")\n",
    "        \n",
    "        return params, countries\n",
    "\n",
    "params, countries = asyncio.get_event_loop().run_until_complete(explore_api())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore a Specific City (Delhi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def explore_city(city=\"Delhi\"):\n",
    "    async with OpenAQClient() as client:\n",
    "        locations = await client.get_locations(\n",
    "            city=city,\n",
    "            parameters=[\"pm25\", \"pm10\", \"no2\", \"o3\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"Found {len(locations)} monitoring locations in {city}\")\n",
    "        \n",
    "        for loc in locations[:5]:\n",
    "            print(f\"\\nLocation: {loc['name']} (ID: {loc['id']})\")\n",
    "            coords = loc.get('coordinates', {})\n",
    "            print(f\"  Coordinates: {coords.get('latitude')}, {coords.get('longitude')}\")\n",
    "            print(f\"  Parameters: {[s.get('parameter', {}).get('name') for s in loc.get('sensors', [])]}\")\n",
    "        \n",
    "        return locations\n",
    "\n",
    "delhi_locations = asyncio.get_event_loop().run_until_complete(explore_city(\"Delhi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fetch Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch 7 days of data\n",
    "fetcher = OpenAQDataFetcher()\n",
    "\n",
    "df = asyncio.get_event_loop().run_until_complete(\n",
    "    fetcher.fetch_city_data(\n",
    "        city=\"Delhi\",\n",
    "        parameters=[\"pm25\", \"pm10\", \"no2\", \"o3\"],\n",
    "        days=7\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Pollution Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "params = ['pm25', 'pm10', 'no2', 'o3']\n",
    "titles = ['PM2.5 (μg/m³)', 'PM10 (μg/m³)', 'NO₂ (ppb)', 'O₃ (ppb)']\n",
    "\n",
    "for ax, param, title in zip(axes.flat, params, titles):\n",
    "    if param in df.columns:\n",
    "        # Group by datetime and average across locations\n",
    "        ts = df.groupby('datetime')[param].mean()\n",
    "        ax.plot(ts.index, ts.values, linewidth=0.8)\n",
    "        ax.set_title(title)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Air Quality in Delhi (7 Days)', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly patterns (important for understanding temporal dynamics)\n",
    "df['hour'] = pd.to_datetime(df['datetime']).dt.hour\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# PM2.5 by hour\n",
    "if 'pm25' in df.columns:\n",
    "    hourly_pm25 = df.groupby('hour')['pm25'].mean()\n",
    "    axes[0].bar(hourly_pm25.index, hourly_pm25.values, color='steelblue')\n",
    "    axes[0].set_xlabel('Hour of Day')\n",
    "    axes[0].set_ylabel('PM2.5 (μg/m³)')\n",
    "    axes[0].set_title('Average PM2.5 by Hour')\n",
    "\n",
    "# NO2 by hour (typically shows traffic patterns)\n",
    "if 'no2' in df.columns:\n",
    "    hourly_no2 = df.groupby('hour')['no2'].mean()\n",
    "    axes[1].bar(hourly_no2.index, hourly_no2.values, color='coral')\n",
    "    axes[1].set_xlabel('Hour of Day')\n",
    "    axes[1].set_ylabel('NO₂ (ppb)')\n",
    "    axes[1].set_title('Average NO₂ by Hour (Traffic Pattern)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between pollutants\n",
    "pollutant_cols = [c for c in ['pm25', 'pm10', 'no2', 'o3', 'co', 'so2'] if c in df.columns]\n",
    "corr = df[pollutant_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='RdYlBu_r', center=0, vmin=-1, vmax=1)\n",
    "plt.title('Pollutant Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for Continual Learning\n",
    "\n",
    "For Nested Learning, we treat sequential time periods as separate \"tasks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, history_window=168, forecast_horizon=24):\n",
    "    \"\"\"\n",
    "    Create input-output sequences for time series prediction.\n",
    "    \n",
    "    - history_window: 168 hours = 7 days of hourly input\n",
    "    - forecast_horizon: 24 hours to predict\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    pollutant_cols = [c for c in ['pm25', 'pm10', 'no2', 'o3'] if c in df.columns]\n",
    "    \n",
    "    for loc_id, loc_df in df.groupby('location_id'):\n",
    "        loc_df = loc_df.sort_values('datetime')\n",
    "        values = loc_df[pollutant_cols].values\n",
    "        \n",
    "        for i in range(len(values) - history_window - forecast_horizon + 1):\n",
    "            x = values[i:i + history_window]\n",
    "            y = values[i + history_window:i + history_window + forecast_horizon]\n",
    "            sequences.append((x, y))\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "sequences = create_sequences(df)\n",
    "print(f\"Created {len(sequences)} training sequences\")\n",
    "\n",
    "if sequences:\n",
    "    x, y = sequences[0]\n",
    "    print(f\"Input shape: {x.shape} (7 days hourly)\")\n",
    "    print(f\"Output shape: {y.shape} (24 hour forecast)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Task Creation for Continual Learning\n",
    "\n",
    "Split data into sequential tasks to simulate streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tasks(df, n_tasks=4):\n",
    "    \"\"\"\n",
    "    Split data chronologically into tasks.\n",
    "    Each task represents a different time period.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('datetime')\n",
    "    total_days = (df['datetime'].max() - df['datetime'].min()).days\n",
    "    days_per_task = total_days // n_tasks\n",
    "    \n",
    "    tasks = []\n",
    "    start_date = df['datetime'].min()\n",
    "    \n",
    "    for i in range(n_tasks):\n",
    "        end_date = start_date + pd.Timedelta(days=days_per_task)\n",
    "        task_df = df[(df['datetime'] >= start_date) & (df['datetime'] < end_date)]\n",
    "        tasks.append(task_df)\n",
    "        start_date = end_date\n",
    "        print(f\"Task {i+1}: {len(task_df)} samples\")\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "# For a proper experiment, you'd use more data\n",
    "# tasks = create_tasks(df, n_tasks=4)\n",
    "print(\"\\n[Note: With only 7 days of data, task creation is limited.\")\n",
    "print(\"For real experiments, fetch 90+ days of data.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Fetch more data**: Use `--days 90` in the fetch script for meaningful continual learning\n",
    "2. **Preprocess**: Handle missing values, normalize, create temporal features\n",
    "3. **Train HOPE-Air**: Run the training script with continual learning\n",
    "4. **Evaluate forgetting**: Measure BWT/FWT metrics across tasks\n",
    "\n",
    "See `notebooks/04_hope_training.ipynb` for the training walkthrough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
